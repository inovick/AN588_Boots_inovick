---
title: "inovick_finalHWcode_05"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, error = TRUE)
```

#First, load the packages needed:
```{r}
library(curl)
library(ggplot2)
library(gridExtra)
library(lmodel2)
```
#Then, load the data using the curl command:
```{r}
f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN588_Fall21/KamilarAndCooperData.csv")
d <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = FALSE)
head(d)
```

#Challenge 1: Using the “KamilarAndCooperData.csv” dataset, run a linear regression looking at log(HomeRange_km2) in relation to log(Body_mass_female_mean) and report your β coeffiecients (slope and intercept).
```{r}
#Assigning variables:
homerange <- log(d$HomeRange_km2)
bodymassf <- log(d$Body_mass_female_mean)

f <- cbind(d, homerange, bodymassf)
m <- lmodel2(homerange ~ bodymassf, data = f, range.y = "interval", range.x = "relative", 
    nperm = 1000)

m

b1 <- m$regression.results$Slope[1]
b1 ##Slope = 1.036, same as my first time. Trying model II regression this time based off peer code but same result, so probably didn't need to do that
b0 <- m$regression.results$Intercept[1]
b0 ##Intercept = -9.44, also same as first time

```
##Question: Is the default CI 97.5 or am I misunderstanding?


#Challenge 2: Then, use bootstrapping to sample from your data 1000 times with replacement, each time fitting the same model and calculating the same coefficients. This generates a sampling distribution for each β coefficient.

##First create the sample dataset:

```{r}
samp.1 <- NULL
samp.2 <- NULL
for (i in 1:1000) {
    samp.1[[i]] <- sample(f$homerange, size = 200, replace = TRUE)
    samp.2[[i]] <- sample(f$bodymassf, size = 200, replace = TRUE)
}
## Sampling 1000 times with replacement from each variable, with a sample size of 200

mm <- NULL
beta1 <- NULL
beta0 <- NULL
for (i in 1:1000) {
  mm[[i]] <- lmodel2(samp.1[[i]]~samp.2[[i]],data = f, range.y = "interval", range.x = "relative")
  beta1[i] <- mm[[i]]$regression.results$Slope[1]
  beta0[i] <- mm[[i]]$regression.results$Intercept[1]
}
## Using Diego's code: Running a for loop for doing the model 2 regression of each sample. I get the warning that "No permutation test will be performed" on each regression because if I add a permutation number the loop takes forever to run. I also get all the slopes and intercepts stored as vectors in the variables beta1 and beta0.
```

```{r}
mm[[1]]
mm[[1000]] ##error says "subscript out of bounds" and I don't know why

mmi <- NULL
for (i in 1:1000) {
  mmi[[i]] <- lm(samp.1[[i]]~samp.2[[i]])
}
mmi[[1]] ##intercept = -5.03, slope = 0.43
mmi[[1000]] ##intercept = -1.61, slope = 0.05
```

##Comparing original values with bootstrapping values. Estimate the standard error for each of your β coefficients as the standard deviation of the sampling distribution from your bootstrap and determine the 95% CI for each of your β coefficients based on the appropriate quantiles from your sampling distribution.
```{r}
sd0 <- sd(beta0) ## Calculates the estimation of the standard error for β0 coefficient 
sd1 <- sd(beta1) ## Calculates the estimation of the standard error for β1 coefficient 
```

##Determine the 95% CI for each of your β coefficients based on the appropriate quantiles from your sampling distribution.
```{r}
##Calculating SE and 95% CIs of the coefficients using the whole dataset for comparing them to the ones obtained by bootstrapping
mi <- lm(homerange ~ bodymassf)
u <- coef(summary(mi))
u <- data.frame(unlist(u))

colnames(u) <- c("Estimate", "SE", "t", "p") ## Creating containers for the coefficients calculated with lm()

ci <- confint(mi, level = 0.95)  # This calculates the CIs for the slope (β1) and the intercept (β0)
t <- data.frame(Bootstrapping_SE = c(sd0, sd1), Bootstrapping_2.5 = c(quantile(beta0, 0.025),quantile(beta1, 0.025)), Bootstrapping_97.5 = c(quantile(beta0, 0.975),quantile(beta1, 0.975))) ## Creating a data frame with all the data obtained by bootstrapping

table <- cbind(u[,1:2],t,ci) ## This gathers the different data frames into a single one
table

knitr::kable(rbind(
  c(u[,1:2],t,ci)))
```

```{r}
#code from https://towardsdatascience.com/bootstrap-regression-in-r-98bfe4ff5007

knitr::kable(rbind(
  c(u[,1:2],t,ci)))

#Table did not print as nicely this time, I probably missed something, so I used the cbind function above instead of rbind
```


##How does the former compare to the SE estimated from your entire dataset using the formula for standard error implemented in lm()?
# The standard errors aren't the same but are not insanely different. The ones calculated by bootstrapping are higher. 


##How does the latter compare to the 95% CI estimated from your entire dataset?
# CIs are pretty different. Could it be due to the inherent function of bootstrapping?
